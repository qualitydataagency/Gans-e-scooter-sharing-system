# Gans-e-scooter-sharing-system
Data Engineering and creating Automated Data Pipeline for E scooter sharing company. Fully Automated Data Pipeline.

Key words: MySQL, Relational data base management systems (RDBMS), Python, Pandas, BeautifulSoup, API, Amazon web services (AWS), Cloud technologies, Data automatization, Predictive modelling, WEB scraping.

Data Engineering and creating Automated Data Pipeline was realy challenging and inspiring WBS Cooding School Project which resulted in fully automated data collection process. Our project was defined by client´s business request that was actualy our project objective.

Client request — project objective

“Based on weather conditions and flights data arrivals, optimise E-scooter locations and predict its placement >Just in time and just in place<”

From the beginning of the project it was clear that it will be challenging while it combines different technologies including: web data scraping from several different sources; different methods and modules i.e. API, Beautifull soup; Relational data base management systems and softwares (Python, MySQL); Cloud Data Base Management Systems and others.

Our strength in this project was readiness and commitment to provide reliable and scalable data pipeline solution in accordance with client expectations.

Conclusions and lessons learned

Automated data pipelines solutions are the processes between different data basis that periodically and automatically move the data from one data base to another

Syntax is not always the same in notebook i.e. JupyterLab and in AWS Lambda, be careful before deploying four scripts in AWS Lambda and check for indentation

Always take care about data reliability and data copyright

Think about scalability of your solutions always in the beginning of the project

Factorizing is useful Python method used to generate unique integer data type from text data type column

Data transformations (cleaning, filtering, and aggregating the data) has to be done first locally on your machine before deploying data to AWS Lambda environment

Python SQL Alchemy method define data flow process which transfer data between two data base and it can save costs by applying different data transfer options https://stackoverflow.com/questions/70652544/best-way-to-move-data-between-two-databases-using-sqlalchemy

Serverless computing and automation with AWS Lambda and Amazon EventBridge is creating amazing experience and up to date data pipeline solution.
